{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a document-based question answering system by using Docling with Granite 3.1\n",
    "\n",
    "**Authors:** Ash Minhas, Anna Gutowska\n",
    "\n",
    "In this tutorial, you will use IBM® [Docling](https://github.com/DS4SD/docling) and open-source [Granite™ 3.1](https://www.ibm.com/granite) to perform document visual question answering for various file types. \n",
    "\n",
    "## What is Docling? \n",
    "\n",
    "Docling is an IBM open-source toolkit for parsing documents and exporting them to preferred formats. Input file formats include PDF, DOCX, PPTX, XLSX, Images, HTML, AsciiDoc and Markdown. These documents can be exported to markdown or JSON. Docling also provides [OCR (optical character recognition)](https://www.ibm.com/think/topics/optical-character-recognition) support for scanned documents. Use cases include scanning medical records, banking documents and even travel documents for quicker processing. \n",
    "\n",
    "## RAG and large context windows\n",
    "\n",
    "[Retrieval augmented generation (RAG)](https://www.ibm.com/think/topics/retrieval-augmented-generation) is an architecture for connecting [large language models (LLMs)](https://www.ibm.com/topics/large-language-models) with external knowledge bases without [fine-tuning](https://www.ibm.com/topics/fine-tuning) or retraining. Text is embedded, stored in a vector database and finally, is used by the pre-trained model to return relevant information for [natural language processing (NLP)](https://www.ibm.com/think/topics/natural-language-processing) and [machine learning](https://www.ibm.com/topics/machine-learning) tasks. \n",
    "\n",
    "When an LLM has a larger [context window](<https://www.ibm.com/think/topics/context-window#:~:text=The%20context%20window%20(or%20%E2%80%9Ccontext,of%20information%20into%20each%20output.>), the generative AI model can process more information at once. This means that we can use both RAG and models with large context windows to leverage the ability to efficiently process more relevant information at a time. The LLM we use in this tutorial is the IBM `Granite-3.1-8B-Instruct` model. This model extends to a context window size of 128K tokens. We will access the model locally by using [Ollama](https://ollama.com/), without the use of an [API](https://www.ibm.com/topics/api). This model is also available on [Hugging Face](https://huggingface.co/ibm-granite). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_Note_:  This is a bad workaround for the following issue:\n",
    ">`OMP: Error #15`: Initializing `libomp140.x86_64.dll`, but found `libiomp5md.dll` already initialized.\n",
    ">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://openmp.llvm.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "This tutorial can be found on our GitHub in the form of a Jupyter Notebook.  Jupyter Notebooks are widely used within [data science](https://www.ibm.com/topics/data-science) to combine code, text, images and [data visualizations](https://www.ibm.com/topics/data-visualization) to formulate a well-formed analysis.\n",
    "\n",
    "### Step 1. Set up your environment\n",
    "We first need to set up our environment by fulfilling some prerequisites. \n",
    "\n",
    "1. Install the latest version of [Ollama](https://ollama.com/) to run locally.\n",
    "\n",
    "2. Pull the latest Granite 3.1 model by running the following command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Install and import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 0a922eb99317... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 4.9 GB                         \n",
      "pulling f7b956e70ca3... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   69 B                         \n",
      "pulling f76a906816c4... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling 492069a62c25... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  11 KB                         \n",
      "pulling e026ee8ed889... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  491 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 274 MB                         \n",
      "pulling c71d239df917... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  11 KB                         \n",
      "pulling ce4a164fc046... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   17 B                         \n",
      "pulling 31df23ea7daa... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull granite3.1-dense:8b\n",
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q \"langchain>=0.1.0\" \"langchain-community>=0.0.13\" \"langchain-core>=0.1.17\" \\\n",
    "    \"langchain-ollama>=0.0.1\" \"pdfminer.six>=20221105\" \"markdown>=3.5.2\" \"docling>=2.0.0\" \\\n",
    "    \"beautifulsoup4>=4.12.0\" \"unstructured>=0.12.0\" \"chromadb>=0.4.22\" \"faiss-cpu>=1.7.4\" \\\n",
    "    \"requests>=2.32.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garcm0b\\Work\\DocAnswerRAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Docling imports\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TesseractCliOcrOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Document format detection\n",
    "\n",
    "We will work with various document formats in this tutorial. Let's create a helper function to detect document formats by using the file extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_format(file_path) -> InputFormat:\n",
    "    \"\"\"Determine the document format based on file extension\"\"\"\n",
    "    try:\n",
    "        file_path = str(file_path)\n",
    "        extension = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "        format_map = {\n",
    "            '.pdf': InputFormat.PDF,\n",
    "            '.docx': InputFormat.DOCX,\n",
    "            '.doc': InputFormat.DOCX,\n",
    "            '.pptx': InputFormat.PPTX,\n",
    "            '.html': InputFormat.HTML,\n",
    "            '.htm': InputFormat.HTML\n",
    "        }\n",
    "        return format_map.get(extension, None)\n",
    "    except:\n",
    "        return \"Error in get_document_format: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Document conversion\n",
    "\n",
    "Next, we can use the `DocumentConverter` class to create a function that converts any supported document to markdown. This function identifies text, data tables, document images and captions by using Docling. The function takes a file as input, processes it using Docling's advanced document handling, converts it to markdown and saves the results in a Markdown file. Both scanned and text-based documents are supported and document structure is preserved. Key components of this function are:\n",
    "- `PdfPipelineOptions`: Configures how PDFs are processed.\n",
    "- `TesseractCliOcrOptions`: Sets up OCR for scanned documents.\n",
    "- `DocumentConverter`: Handles the actual conversion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_document_to_markdown(doc_path) -> str:\n",
    "    \"\"\"Convert document to markdown using simplified pipeline\"\"\"\n",
    "    try:\n",
    "        # Convert to absolute path string\n",
    "        input_path = os.path.abspath(str(doc_path))\n",
    "        print(f\"Converting document: {doc_path}\")\n",
    "\n",
    "        # Create temporary directory for processing\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Copy input file to temp directory\n",
    "            temp_input = os.path.join(temp_dir, os.path.basename(input_path))\n",
    "            shutil.copy2(input_path, temp_input)\n",
    "\n",
    "            # Configure pipeline options\n",
    "            pipeline_options = PdfPipelineOptions()\n",
    "            pipeline_options.do_ocr = False  # Disable OCR temporarily\n",
    "            pipeline_options.do_table_structure = True\n",
    "\n",
    "            # Create converter with minimal options\n",
    "            converter = DocumentConverter(\n",
    "                allowed_formats=[\n",
    "                    InputFormat.PDF,\n",
    "                    InputFormat.DOCX,\n",
    "                    InputFormat.HTML,\n",
    "                    InputFormat.PPTX,\n",
    "                ],\n",
    "                format_options={\n",
    "                    InputFormat.PDF: PdfFormatOption(\n",
    "                        pipeline_options=pipeline_options,\n",
    "                    ),\n",
    "                    InputFormat.DOCX: WordFormatOption(\n",
    "                        pipeline_cls=SimplePipeline\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Convert document\n",
    "            print(\"Starting conversion...\")\n",
    "            conv_result = converter.convert(temp_input)\n",
    "\n",
    "            if not conv_result or not conv_result.document:\n",
    "                raise ValueError(f\"Failed to convert document: {doc_path}\")\n",
    "\n",
    "            # Export to markdown\n",
    "            print(\"Exporting to markdown...\")\n",
    "            md = conv_result.document.export_to_markdown()\n",
    "\n",
    "            # Create output path\n",
    "            output_dir = os.path.dirname(input_path)\n",
    "            base_name = os.path.splitext(os.path.basename(input_path))[0]\n",
    "            md_path = os.path.join(output_dir, f\"{base_name}_converted.md\")\n",
    "\n",
    "            # Write markdown file\n",
    "            print(f\"Writing markdown to: {base_name}_converted.md\")\n",
    "            with open(md_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(md)\n",
    "\n",
    "            return md_path\n",
    "    except:\n",
    "        return f\"Error converting document: {doc_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. QA chain setup\n",
    "\n",
    "The QA chain is the heart of our system. It combines several components:\n",
    "\n",
    "1. Document loading:\n",
    "- Loads the markdown file that we created.\n",
    "- Splits it into manageable chunks for processing.\n",
    "\n",
    "2. Text splitting:\n",
    "- Breaks down the document into smaller pieces.\n",
    "- Maintains context with overlap between chunks.\n",
    "- Ensures efficient processing by the language model.\n",
    "\n",
    "3. Vector store:\n",
    "- Creates embeddings for each text chunk.\n",
    "- Stores them in a FAISS index for fast retrieval.\n",
    "- Enables semantic search capabilities.\n",
    "\n",
    "4. Language model:\n",
    "- Uses Ollama for both embeddings and text generation.\n",
    "- Maintains conversation history.\n",
    "- Generates contextual responses.\n",
    "\n",
    "The following `setup_qa_chain` function sets up this entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain(markdown_path: Path, embeddings_model_name:str = \"nomic-embed-text:latest\", model_name: str = \"granite3.1-dense:8b\"):\n",
    "    \"\"\"Set up the QA chain for document processing\"\"\"\n",
    "    # Load and split the document\n",
    "    loader = UnstructuredMarkdownLoader(str(markdown_path)) \n",
    "    documents = loader.load()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    # texts= documents\n",
    "    \n",
    "    # Create embeddings and vector store\n",
    "    embeddings = OllamaEmbeddings(\n",
    "        model=embeddings_model_name\n",
    "        )\n",
    "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "    \n",
    "    # Initialize LLM\n",
    "    llm = OllamaLLM(\n",
    "        model=model_name,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Set up conversation memory\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True\n",
    "    )\n",
    "    \n",
    "    # Create the chain\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": 10}\n",
    "            ),\n",
    "        memory=memory,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Set up question-answering interface\n",
    "\n",
    "Finally, let's create a simple interface for asking questions. This function takes in the chain and user query as parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(qa_chain, question: str):\n",
    "    \"\"\"Ask a question and display the answer\"\"\"\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    display(Markdown(f\"**Question:** {question}\\n\\n**Answer:** {result['answer']}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Perform question-answering\n",
    "\n",
    "Let's put it all together and enumerate over our questions for a specific document. The path to this document is stored in `doc_path` and can be any document you want to test. For our sample document, check out our GitHub. The system maintains conversation history and can handle follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting document: Stochastic_Parrots.pdf\n",
      "Starting conversion...\n",
      "Exporting to markdown...\n",
      "Writing markdown to: Stochastic_Parrots_converted.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garcm0b\\AppData\\Local\\Temp\\ipykernel_54804\\701686706.py:28: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** What is the main topic of this document?\n",
       "\n",
       "**Answer:** The main topic of this document revolves around responsible practices in the research and development of language technology. It emphasizes the importance of considering human impacts, environmental consequences, data curation, documentation, stakeholder engagement, and ethical considerations when creating such technologies. The authors advocate for a broad view on potential effects of technology on people and communities, particularly those that may be adversely affected. They also discuss the challenges associated with large datasets based on Internet texts, including overrepresentation of hegemonic viewpoints and encoding biases harmful to marginalized populations. The document encourages researchers to budget for curation and documentation at the beginning of a project and explore alternative research directions beyond ever-larger language models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** What are the key points discussed?\n",
       "\n",
       "**Answer:** 1. The document emphasizes the need for careful planning in language technology research and development, considering various dimensions to mitigate risks associated with increasingly large language models (LMs).\n",
       "\n",
       "2. It advocates for a mindset that centers on the people who may be adversely affected by the resulting technology, taking into account environmental impacts, data curation, documentation, and stakeholder engagement early in the design process.\n",
       "\n",
       "3. The authors warn about the risks of ingesting everything from the web, as it can lead to overrepresentation of hegemonic viewpoints and encoding biases potentially damaging to marginalized populations. They recommend budgeting for curation and documentation at the start of a project.\n",
       "\n",
       "4. The document discusses how large datasets based on texts from the Internet may incur \"documentation debt,\" making it difficult to understand what is in the training data as model size increases.\n",
       "\n",
       "5. It highlights the unequal distribution of risks and benefits, with marginalized communities often bearing the brunt of negative consequences, such as environmental racism.\n",
       "\n",
       "6. The authors explore real-world risks associated with deploying language technologies that encode hegemonic worldviews, amplify biases, and are mistaken for actual natural language understanding.\n",
       "\n",
       "7. They suggest reevaluating the primary driver of increased performance in language technology, moving away from relying solely on ever-increasing size of LMs towards approaches that avoid some of these risks while still benefiting from improvements to language technology.\n",
       "\n",
       "8. The document references a critical survey on 'bias' in NLP by Su Lin Blodgett et al., published in the Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (2020)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Question:** Can you summarize the conclusions?\n",
       "\n",
       "**Answer:** The main conclusions drawn from this document regarding responsible practices in language technology research and development are as follows:\n",
       "\n",
       "1. Hegemonic viewpoints and biases: Large datasets based on texts from the Internet tend to overrepresent hegemonic worldviews and encode biases that can be potentially damaging to marginalized populations (§4).\n",
       "\n",
       "2. Documentation debt: The risk of incurring documentation debt increases with the size of models, making it difficult to understand what is in the training data (§4).\n",
       "\n",
       "3. Careful planning: Researchers should adopt a mindset of careful planning before starting to build datasets or systems trained on them, considering various dimensions such as environmental impact, human impacts, and ethical implications (§7).\n",
       "\n",
       "4. Environmental consequences: The environmental impact scales with model size, and the document encourages weighing the environmental costs first when developing language technology (§6).\n",
       "\n",
       "5. Data curation and documentation: Investing resources into curating and carefully documenting datasets is recommended instead of ingesting everything on the web (§7).\n",
       "\n",
       "6. Pre-development exercises: Conducting pre-development exercises to evaluate how the planned approach fits into research and development goals and supports stakeholder values is encouraged (§7).\n",
       "\n",
       "7. Stakeholder engagement: Engaging with stakeholders, including marginalized communities, is essential to ensure that the benefits and risks of language technology are distributed fairly (§6).\n",
       "\n",
       "8. Ethical implications: The document highlights the need to consider ethical implications, such as the potential for amplifying biases in training data, when developing language technology (§5).\n",
       "\n",
       "9. Risk/benefit analysis: When performing risk/benefit analyses of language technology, it is crucial to keep in mind how risks and benefits are distributed, as they do not accrue to the same people (§6).\n",
       "\n",
       "10. Avoiding over-reliance on large models: The document suggests that relying solely on ever-increasing size of LMs as the primary driver of increased performance may lead to unintended consequences and risks, and encourages exploring alternative research directions beyond larger language models (§7)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process a document\n",
    "doc_path = Path(\"Stochastic_Parrots.pdf\")  # Replace with your document path\n",
    "\n",
    "# Check format and process\n",
    "doc_format = get_document_format(doc_path)\n",
    "if doc_format:\n",
    "    md_path = convert_document_to_markdown(doc_path)\n",
    "    qa_chain = setup_qa_chain(md_path)\n",
    "    \n",
    "    # Example questions\n",
    "    questions = [\n",
    "        \"What is the main topic of this document?\",\n",
    "        \"What are the key points discussed?\",\n",
    "        \"Can you summarize the conclusions?\",\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        ask_question(qa_chain, question)\n",
    "else:\n",
    "    print(f\"Unsupported document format: {doc_path.suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The system was able to retrieve relevant information from the document to answer questions. Feel free to test this system with any of your own files and questions!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Using Docling and Granite 3.1, you built a document question answering system compatible with various file types. As a next step, this methodology can be applied to a [chatbot](https://www.ibm.com/topics/chatbots) with an interactive UI. There are many opportunities to transform this tutorial to apply to specific use cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
